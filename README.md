# ğŸŒŸ NeuroXOR: AI-Powered XOR Classifier

ğŸ¯ **A Lightweight Neural Network for XOR Classification** ğŸ§ 

---

## ğŸš€ Project Overview
**NeuroXOR** is a compact yet powerful feedforward neural network written in C++ to classify the XOR function. It showcases key neural network concepts such as forward propagation, activation functions, and model persistence (saving/loading). This project is an excellent entry point for learning neural networks in a low-level language!

ğŸ›  **Built With:**
- **C++** â€“ High-performance execution
- **Mathematics** â€“ Applied linear algebra & calculus
- **Machine Learning** â€“ Neural networks & activation functions
- **File Handling** â€“ Save & load trained models

---

## ğŸŒˆ Features
âœ… **Multiple Activation Functions:** Choose between Sigmoid, ReLU, Leaky ReLU, and Tanh.  
âœ… **Optimized Weight Initialization:** Implements **Xavier/He initialization** for better convergence.  
âœ… **Model Persistence:** Save & load trained models seamlessly.  
âœ… **Structured Neural Network:** Modular, scalable, and easy to extend.  
âœ… **Customizable Hyperparameters:** Fine-tune learning rates, epochs, and batch sizes.  

---

## ğŸ”§ Installation & Usage
To compile and run the project, ensure you have a **C++ compiler** (such as g++ or clang++) installed.

```bash
# Compile the program
g++ -o neuroxor neuroxor.cpp -std=c++11

# Run the executable
./neuroxor
```

ğŸ“Œ **Pro Tip:** Experiment with different activation functions and hyperparameters to see how the network behaves!

---

## âš™ï¸ How It Works
1ï¸âƒ£ **Data Handling:** Uses XOR gate training data.  
2ï¸âƒ£ **Weight Initialization:** Implements He initialization for stability.  
3ï¸âƒ£ **Forward Propagation:** Computes hidden and output layers using the selected activation function.  
4ï¸âƒ£ **Model Persistence:** Saves and loads model weights & biases to avoid retraining.  

ğŸ“ **Mathematical Representation:**
\[
y = f(W_2 \cdot f(W_1 \cdot x + b_1) + b_2)
\]

Where:
- \( W_1, W_2 \) = Weight matrices
- \( b_1, b_2 \) = Bias vectors
- \( f \) = Activation function
- \( x \) = Input vector

---

## ğŸ¨ Activation Functions & Their Formulas
| Function | Formula | Best Use Case |
|----------|---------|--------------|
| **Sigmoid** | \( \sigma(x) = \frac{1}{1 + e^{-x}} \) | Binary classification |
| **ReLU** | \( f(x) = max(0, x) \) | Deep networks |
| **Leaky ReLU** | \( f(x) = x > 0 ? x : 0.01x \) | Avoiding dead neurons |
| **Tanh** | \( f(x) = tanh(x) \) | Zero-centered data |

ğŸ”¬ **Try different activation functions and observe their impact on training!**

---

## ğŸ“‚ Project Structure
```
ğŸ“ NeuroXOR/
â”‚â”€â”€ ğŸ“œ neuroxor.cpp   # Main source code
â”‚â”€â”€ ğŸ“„ model.dat      # Saved model weights and biases
â”‚â”€â”€ ğŸ“– README.md      # Documentation
```

---

## ğŸ“Š Example Output
After training, the neural network should correctly classify XOR inputs:
```bash
Input: (0,0) -> Output: ~0.0
Input: (0,1) -> Output: ~1.0
Input: (1,0) -> Output: ~1.0
Input: (1,1) -> Output: ~0.0
```

ğŸ” **Experiment with hyperparameters and observe how accuracy improves!**

---

## ğŸš€ Future Enhancements
âœ¨ Implement backpropagation & gradient descent.  
âœ¨ Expand dataset & improve generalization.  
âœ¨ Introduce batch training for faster convergence.  
âœ¨ Add visualization support using Python & Matplotlib.  

---

## ğŸ¤ Contributing
Want to improve **NeuroXOR**? Fork the repository, make your changes, and submit a pull request! ğŸš€

---

## ğŸ“œ License
ğŸ“ This project is open-source and available under the **MIT License**. Enjoy coding! ğŸ‰

